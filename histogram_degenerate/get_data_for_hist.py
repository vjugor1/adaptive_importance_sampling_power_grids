# Import namespaces
import sys
import os
import math
import itertools
import json
import numpy as np
import scipy as scp
import matplotlib.pyplot as plt
import scipy.linalg as la
import pandapower as pp


from scipy.stats import norm
from scipy.stats import uniform
from scipy.stats import multinomial
from scipy.linalg import pinvh
from pandapower import networks

from tqdm.notebook import tqdm, trange
import time 


def gen_data_for_hist(J = 700, tau = 2, eps_plane_m = 1e-8):
    ##### Setup Grid fluctuation parameters and constraints ########

    ## Std considered to be 1: 

    cov_std = 1

    ### Number of samples used in experiments
    ### 500 is often enough
    ### 10000 is a default value supresses the variance

    #nsmp = 10000; 
    nsmp = 100; 


    ### Step-sizes for KL and Var minimization 
    ### works well with 0.1-0.01

    eta_vm = 0.1; 
    eta_kl = 0.1; 

    ### Rounding threshold in optimization: 
    ### if a (normalized on the simplex) hpl probability becomes lower then 0.001
    ### we increase it to this level
    ###
    ### Crucially affects numerical stability
    ###

    eps = 0.001
    
    ##### Setup regular polytope ########
    ### number of planes and distance
    J = J
    tau = tau
    eps_plane_m = 1e-8


    ### Matrix 
    x_An = [[0.0, 1.0]]
    ### Matrix rows
    for i in range(J-1):
        eps_plane = np.random.uniform(- eps_plane_m, eps_plane_m)
        x_An.append([eps_plane, -1.0 + eps_plane])
    x_An = np.array(x_An)
    ### Distances
    x_bn = np.ones(J) * tau
    n = x_An.shape[1]
    
    ### Compute probabilities:
    ### prb: probability of each hpl failure 
    ### p_up, p_dwn: upper and lower bounds
    ###

    prb = norm.cdf(-x_bn);
    p_up = np.sum(prb);
    p_dwn = np.max(prb);

    print("the union bound (upper):", p_up)
    print("the max bound (lower):", p_dwn)
    print("approximate target (2 x lower):", p_dwn * 2)
    
    ############# Monte-Carlo ##################

    rv = norm(); 
    x_std = norm.rvs(size=[n,nsmp])
    smp = x_An@x_std

    ### fls_mc = failures in Monte-Carlo, e.g. 
    ### when MC discovers a failure
    ###

    ### probability estimates history
    mc_exp_history = []
    mc_std_history = []

    fls_mc = sum((x_bn <= smp.T[:]).T); 
    print("Max # of hlps a sample if out of: ", np.max(fls_mc))

    ### MC failure expectation and std
    ###

    for i in range(nsmp):
        mc_exp_history.append((1 - np.sum(fls_mc[:i] == 0)/(i + 1)))
        mc_std_history.append(np.std(mc_exp_history))

    mc_exp = (1 - np.sum(fls_mc == 0)/nsmp);
    mc_std = mc_std_history[-1];
    # violation_dict = {}
    # for i in range(0,np.max(fls_mc)+1):
    #     print(i, "hpls violated (exactly) vs # cases",  np.sum(fls_mc == i))
    #     violation_dict[i] = int(np.sum(fls_mc == i))

    print("\nMC(exp, std):", (mc_exp, mc_std)); 

    ### write into file
    # path_to_viol_dirs = os.path.join("results", "hplns_violations")
    # with open(os.path.join(path_to_viol_dirs, "grid3120", "result.json"), 'w+') as fp:
    #     json.dump(violation_dict, fp)
    
    ############# ALOE ##################
    ### 
    ### Exactly follows to the Owen/Maximov/Chertkov paper, EJOS'19
    ###
    ### sample z ~ N(0, I_n)
    ### sample u ~ U(0,1)
    ### compute y = F^{-1}(u F(-b_i))
    ### compute x = - (a_i * y + (I - a_i.T * a_i) z)
    ###
    ### Ouput: union bound divided by the expected failure multiplicity
    ###


    ### Initialize samplers
    ###
    ### sample z ~ N(0, I_n) and u ~ U(0,1)
    ### 

    rv = norm();
    rv_u = uniform();
    z = norm.rvs(size=[nsmp,n]); 
    u = uniform.rvs(size=[nsmp]);


    ### x_alph is a vector of ALOE probabilities 
    ### normalized by a unit simplex
    ###

    x_alph = prb/np.sum(prb);
    #print("ALOE prbs for major hpls: ", x_alph)

    ### _hpl: how many smpls beyond each of the hpls
    ###

    _hpl = multinomial.rvs(n = nsmp, p = x_alph);

    ### print("# samples per hpl", _hpl)

    ### Get cummulative sums, which are easier to work with
    _hpl = list(itertools.accumulate(_hpl))
    _hpl = np.array(_hpl)

    ### print("cusum of # hpls", _hpl)

    ### Generate samples
    ### x_aloe -- samples generated by ALOE
    ###
    ### TODO: seems optimizable, but I am not sure about memory mgmnt in python
    x_aloe = np.zeros([nsmp, n]); 

    # index of the active hyperplane
    hpl_id = 0;

    ### get samples x_aloe according to the algorithm
    #for i in tqdm(range(0,nsmp)):
    for i in range(0,nsmp):
        ### get index of a hyperplane to sample beyond
        hpl_id = (hpl_id, hpl_id+1)[i >= _hpl[hpl_id]];
        y = norm.ppf(u[i]*norm.cdf(-x_bn[hpl_id]));
        x_aloe[i] = - x_An[hpl_id]*y - z[i] + np.outer(x_An[hpl_id],x_An[hpl_id])@z[i];

    ### test how many constraints are violated
    smp = x_An@x_aloe.T

    ### compute expectation and std final and history
    aloe_exp = p_up*np.sum(1./np.sum(x_bn <= smp.T[:],axis=1))/nsmp; 
    aloe_std = p_up*math.sqrt(2*len(_hpl))/math.sqrt(nsmp); # indeed len(_hpl) instead of 2*m in the Thrm
    aloe_exp_history = [p_up * np.sum(1. / np.sum(x_bn <= (x_An@x_aloe[:i, :].T).T, axis=1)) / (i + 1) for i in range(0,nsmp)]
    aloe_std_history = [p_up*math.sqrt(2*len(_hpl))/math.sqrt(i + 1) for i in range(0, nsmp)]


    print("ALOE (exp, std)", (aloe_exp, aloe_std))
    
    ####### Optimization approach ######
    #######
    ####### Variance Minimization ######
    ####### 


    ### setup the initial values

    eta = eta_vm; 
    md_var = 0; 
    md_exp = 0;
    grad = np.zeros(len(x_bn)); #gradient on each iteration
    _hpl = np.zeros(nsmp); # hpls choosen by the method 

    ### intentionally use a copy instead of a reference
    ### alph is a vector of weigths to be updated in algorithm
    ###

    alph = x_alph[:];

    ### history of probability estimate and std
    md_exp_history = []
    md_std_history = []

    # values for Phi (x_bn)
    x_phi = [norm.cdf(-x_bn[i]) for i in range(0, len(x_bn))]

    ### grad normalization by prbs[i] factor is introduced to make computations numerically stable
    ###

    prbs = prb;

    for i in tqdm(range(0,nsmp)):

        ### sample x according to current alph
        hpl_id = np.where(multinomial.rvs(n=1,p = alph, size=1, random_state=None)[0] == 1)[0];
        _hpl[i] = hpl_id;        

        ### generate a sample following to the ALOE procedure
        y = norm.ppf(u[i]*norm.cdf(-x_bn[hpl_id]));

        x_smp = - x_An[hpl_id]*y - z[i] + np.outer(x_An[hpl_id],x_An[hpl_id])@z[i];

        ### the RHS' to be compared with x_bn
        x_smp = x_An@x_smp.T;

        ### results of constraints violations for each generated object
        cns_vlt = (x_bn <= x_smp.T[:])[0]

        ### weight vector defined by the multiplicity of constraint violation for each sample
        wgt = 1./np.sum(np.multiply(cns_vlt, np.multiply(alph, 1./x_alph))); 


        ### compute gradient of the variance, see the paper (our + OMC) for details
        grad = [-p_up*p_up*wgt*wgt*norm.pdf(x_smp[k])[0]*cns_vlt[k]/prbs[k] for k in range(len(x_smp))];
        grad = np.array(grad)


        ### The gradient is high -- signal about emergency as it can zero out all weights
        if (la.norm(eta*grad)>1e4):
            print("\n##############    Extremely high gradient      ############\n");
            print("Iteration: ", i, "\nGradient:", grad)

        ### make a ``simplex MD'' update
        alph = [math.exp(-eta*grad[k])*alph[k] for k in range(0,len(x_smp))];


        ### enter if some coordinates are too small and may cause numerical instability
        ### increase the corresponding weigths
        if (np.min(alph) < eps):
            print("###########  some coordinates are small  #################")
            alph = [alph[k]+eps for k in range(0,len(x_bn))];

        ### make a projection to the unit simplex
        alph = alph/np.sum(alph); 

        ### adjust contribution to the errors
        md_exp = md_exp + wgt;
        md_exp_history.append(p_up * md_exp / (i + 1))
        md_var = md_var + p_up*np.dot(grad.T,grad);
        md_std_history.append(p_up * math.sqrt(md_var) / (i + 1))


    #print("Optimal weigths of MD-Var minimization: ", alph)
    #print("Optimal weigths of ALOE", x_alph)

    ### normalize errors, compute standard deviation
    md_exp = p_up*md_exp/nsmp; 
    md_std = p_up*math.sqrt(md_var)/nsmp;

    print("MD-Var (exp, std)", (md_exp, md_std))
    #print("assert normalization:", np.sum(alph), np.sum(x_alph))
    
    ####### Optimization approach ######
    #######
    ####### KL Minimization ######
    #######

    ### SMD step-size
    eta = eta_kl; 

    ### setup initial values
    kl_exp = 0; 
    kl_var = 0; 
    grad = np.zeros(len(x_bn));
    _hpl = np.zeros(nsmp); ## _hpl[i] = beyond which hpl we sample on iteration i

    ### intentionally use a copy instead of a reference
    ### alph is an optimization variable
    alph = x_alph[:];

    ### history of probability estimate and std
    kl_exp_history = []
    kl_std_history = []

    ### this normalization factor is introduced to make computations numerically stable
    prbs = prb;


    for i in tqdm(range(0,nsmp)):#,miniters=500):

        ### sample x according to current alph
        hpl_id = np.where(multinomial.rvs(n=1,p = alph, size=1, random_state=None)[0] == 1)[0];
        _hpl[i] = hpl_id;        

        ### generate a sample accordint to ALOE
        y = norm.ppf(u[i]*norm.cdf(-x_bn[hpl_id]));
        x_smp = - x_An[hpl_id]*y - z[i] + np.outer(x_An[hpl_id],x_An[hpl_id])@z[i];

        ### RHS to compare with x_bn
        x_smp = x_An@x_smp.T;

        ### results of constraints violations for the generated object
        cns_vlt = (x_bn <= x_smp.T[:])[0]

        ### object weight which is set according to ALOE
        wgt = 1./np.sum(np.multiply(cns_vlt, np.multiply(alph, 1./x_alph))); 

        # the KL divergence's gradient 
        grad = [-p_up*wgt*norm.pdf(x_smp[k])[0]*cns_vlt[k]/prbs[k] for k in range(len(x_smp))];
        grad = np.array(grad)

        ### The gradient is high -- signal about emergency as it can zero out all weights
        if (la.norm(eta*grad)>1e4):
            print("\n##############    Extremely high gradient      ############\n");
            print("Iteration: ", i, "\nGradient:", grad)

        ### make a ``simplex MD'' update
        alph = [math.exp(-eta*grad[k])*alph[k] for k in range(0,len(x_smp))];

        ### enter if some coordinates are too small and may cause numerical instability
        ### increase the corresponding weigths
        if (np.min(alph) < eps):
            print("###########  some coordinates are small  #################")
            alph = [alph[k]+eps for k in range(0,len(x_bn))];

        ### make a projection to the unit simplex
        alph = alph/np.sum(alph); 

        ### adjust contribution to the errors
        kl_exp = kl_exp + wgt;
        kl_exp_history.append(p_up * kl_exp / (i + 1))
        kl_var = kl_var + p_up*np.dot(grad.T,grad)*wgt;
        kl_std_history.append(p_up * math.sqrt(kl_var) / (i + 1))

    #print("Optimal weigths of MD-KL minimization: ", alph)
    #print("Optimal weigths of ALOE", x_alph)

    ### normalize errors
    kl_exp = p_up*kl_exp/nsmp; 
    kl_std = p_up*math.sqrt(kl_var)/nsmp;
    print("MD-KL (exp, std)", (kl_exp, kl_std))

    #print("assert normalization:", np.sum(alph), np.sum(x_alph))
    
    ##############  Output all probabilities  ##################

    print("the union bound (up):", p_up)
    print("the max bound (lower):", p_dwn)

    print("MC(exp, std):", mc_exp, mc_std); 
    print("ALOE(exp, std)", aloe_exp, aloe_std)

    print("MD-Var(exp, var)", md_exp, md_std)
    print("MD-KL(exp, var)", kl_exp, kl_std)
    
    return 2 * p_dwn, mc_exp, aloe_exp, md_exp, kl_exp

output_dict = {'MC': [], 'MD-Var': [], 'MD-KL': [], 'ALOE': []}
target, mc, aloe, md, kl = gen_data_for_hist(J = 100, tau = 1, eps_plane_m = 1e-6)
output_dict['MC'].append(mc)
output_dict['MD-Var'].append(md)
output_dict['MD-KL'].append(kl)
output_dict['ALOE'].append(aloe)
output_dict['Target'] = target
for i in range(100):
    _, mc, aloe, md, kl = gen_data_for_hist(J = 100, tau = 1, eps_plane_m = 1e-6)
    output_dict['MC'].append(mc)
    output_dict['MD-Var'].append(md)
    output_dict['MD-KL'].append(kl)
    output_dict['ALOE'].append(aloe)
with open('hist_data.json', 'w+') as fp:
    json.dump(output_dict, fp)